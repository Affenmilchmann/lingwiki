{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b7354a",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "`pip install wikipedia`\n",
    "\n",
    "`pip install ipywidgets`\n",
    "\n",
    "`pip install requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253014db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikidownloader as wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1e4bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pages in module wikidownloader:\n",
      "\n",
      "pages(page_amount, buffer_size=50, retry_failed_pages=False) -> Generator[str, NoneType, NoneType]\n",
      "    Generator that downloads and yields string with 'buffer_size' random pages joined in one string.\n",
      "    It loads pages in amount of 'buffer_size' in separate threads and joins them in a string.\n",
      "    So you dont have to wait for 2 seconds for every page to load like if you were using a single thread.\n",
      "    It also loads them while your code is running. So if its relatively slow, the next yield will\n",
      "    probably be ready when you ask for next output\n",
      "    \n",
      "    Note: if 'page_amount' < 'buffer_size' then 'buffer_size' will be set to max(page_amount//100, 1)\n",
      "    \n",
      "     - 'page_amount': amount of random wikipedia pages you want to get. It is guaranteed to\n",
      "     give you at least asked amount. It probably will give you more\n",
      "     - 'buffer_size': amount of pages that will be loaded simultaneously and yielded to you joined \n",
      "     in a single string. (return page amount may be less than buffer_size but in the end you will\n",
      "     get at least 'page_amount' pages. See 'retry_failed_pages' for details)\n",
      "     - 'retry_failed_pages': api is throwing exceptions frequently (around 15% of requests). This \n",
      "     parameter will force api to ask for a new page till it gets it. There is ofc a limit 25 of attempts\n",
      "     for example if you have no internet it will try 25 times to get a page and then return None and log\n",
      "     an exception. So you better set it True when your buffer_size is small. And False if its big. \n",
      "     With False it will return less pages combined but faster bc it doesnt have to retry to get all\n",
      "     the pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wiki.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e7404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 words\t ...\n",
      "1194 words\tThe 1125 German royal election was the ...\n",
      "85 words\tMir Mukhtar Akhyar (1653-1719)(Urdu: میرمختار اخیار) was ...\n",
      "45 words\tWick is an unincorporated community in Tyler ...\n",
      "CPU times: total: 609 ms\n",
      "Wall time: 7.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for page_text in wiki.pages(3):\n",
    "    words = page_text.split(\" \")\n",
    "    print(f\"{len(words)} words\\t{''.join(w+' ' for w in words[:7])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eead375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 words\tАлександр Карл Ангальт-Бернбургский (нем. Alexander Carl von Anhalt-Bernburg; 2 марта ...\n",
      "182 words\tГлисерио Бадильес (исп. Glicerio Badilles) — филиппинский шахматист, национальный мастер.\n",
      "Входил ...\n",
      "241 words\tТенофовир/эмтрицитабин, известный под торговой маркой Truvada — комбинация двух антиретровирусных ...\n",
      "CPU times: total: 547 ms\n",
      "Wall time: 6.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki.set_lang('ru')\n",
    "for page_text in wiki.pages(3):\n",
    "    words = page_text.split(\" \")\n",
    "    print(f\"{len(words)} words\\t{''.join(w+' ' for w in words[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e415b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\пк\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\пк\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19911 words\tЕвгений Петрович Новиков (15 [27] августа 1826, Москва — 21 ...\n",
      "26078 words\tДоротея Энгельбретсдаттер (норв. Dorothe Engelbretsdotter; 16 января 1634, Берген, Норвегия ...\n",
      "16409 words\tМесяцослов — нецерковный календарь, появившийся в России в 1702 году ...\n",
      "23643 words\tДворец Гижицкого — памятник архитектуры в селе Новоселица Хмельницкой области ...\n",
      "23238 words\tСидни А. Монкриф (англ. Sidney A. Moncrief; род. 21 сентября ...\n",
      "33620 words\tФаминцыны —  древний дворянский род.\n",
      "\n",
      "\n",
      "== Происхождение и история рода ...\n",
      "CPU times: total: 1min 34s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# retry_failed_pages=True. It returns almost always exactly 100 pages in a single string but it takes longer\n",
    "for page_text in wiki.pages(300, buffer_size=50, retry_failed_pages=True):\n",
    "    words = page_text.split(\" \")\n",
    "    print(f\"{len(words)} words\\t{''.join(w+' ' for w in words[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f9bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22813 words\tНиже представлена хронология мировых рекордов у мужчин в легкоатлетической эстафете ...\n",
      "38072 words\tГуставо Эспиноса Эспадас младший (исп. Gustavo Espinosa Espadas Jr.; род. ...\n",
      "40041 words\tШайыр (каз. Шайыр) — село в Мангистауском районе Мангистауской области ...\n",
      "23328 words\tТайгер Девор (англ. Tiger Devore; ранее известный как Говард Девор ...\n",
      "17532 words\tВеджини (груз. ვეჯინი) — село в Грузии. Находится в Гурджаанском ...\n",
      "31044 words\tThe Sims 3: Pets — однопользовательская видеоигра в жанре симулятора ...\n",
      "19496 words\tВоронежский институт МВД России — высшее военно-учебное заведение, основанное 29 ...\n",
      "CPU times: total: 1min 30s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# retry_failed_pages=False. It returns less than 100 pages in a single string but it takes faster in a long run.\n",
    "# you will still recieve at least requested amount of pages in total (probably even more)\n",
    "for page_text in wiki.pages(300, buffer_size=50, retry_failed_pages=False):\n",
    "    words = page_text.split(\" \")\n",
    "    print(f\"{len(words)} words\\t{''.join(w+' ' for w in words[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff52fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_widget in module wikidownloader:\n",
      "\n",
      "get_widget() -> ipywidgets.widgets.widget_box.HBox\n",
      "    # Returns ipywidgets.HBox. \n",
      "    its progress bar can be modified with following methods:\n",
      "     - update_bar_val()\n",
      "     - update_bar_max()\n",
      "     - update_bar_desc()\n",
      "     - update_bar_done()\n",
      "     - update_bar_notdone()\n",
      "    \n",
      "    also upper text from the left widget box is editable with\n",
      "     - update_text_status()\n",
      "    \n",
      "    additionaly you may add set text between upper and lower texts in left box\n",
      "     - update_text_info()\n",
      "    \n",
      "    These methods will do nothing if called before get_widget()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wiki.get_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "282a777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de140df584e04ce5ad997196bd7b522b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(margin='auto', width='92%')), Output(layout=Layout(margin='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki.get_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9193168",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_text_status(\"I'm here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2576edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_text_info(\"And I am here too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a20414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_bar_max(150)\n",
    "wiki.update_bar_val(78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd307bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_bar_desc(\"Custom text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0cdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_bar_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe912444",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.update_bar_notdone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26b1db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45402c6d1234f8ea21ec67d9b7be9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(margin='auto', width='92%')), Output(layout=Layout(margin='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki.get_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb0f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3547 words\t1049 (ты́сяча со́рок девя́тый) год  по юлианскому календарю — ...\n",
      "44844 words\tРеспубликанская альтернатива (РЕАЛ) (азерб. Respublikaçı Alternativ Partiyası; англ. Republican Alternative ...\n",
      "8804 words\tCaliban — немецкая металкор-группа. На сегодняшний день выпустила одиннадцать студийных ...\n",
      "4004 words\tFurcifer lateralis  (лат.) — вид ящериц из семейства хамелеонов, ...\n",
      "CPU times: total: 18.1 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for page_text in wiki.pages(50, buffer_size=15):\n",
    "    words = page_text.split(\" \")\n",
    "    print(f\"{len(words)} words\\t{''.join(w+' ' for w in words[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162348c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5c9c327e9d463284b4f3465f18b1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(margin='auto', width='92%')), Output(layout=Layout(margin='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki.get_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e24b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\пк\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\пк\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 17s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# example of practical using with progress bar interaction\n",
    "from time import time\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "pages_amount = 500\n",
    "output_file_name = f'{pages_amount}_pages.txt'; open(output_file_name, 'w').close() # clearing/creating file\n",
    "b_time = time()\n",
    "bar_update_step = 50\n",
    "\n",
    "for pages_text in wiki.pages(pages_amount, buffer_size=pages_amount//10):\n",
    "    # tokenizing\n",
    "    wiki.update_text_status('Tokenizing sentences...')\n",
    "    sentences = sent_tokenize(pages_text)\n",
    "    # widget visuals\n",
    "    wiki.update_bar_notdone()\n",
    "    wiki.update_bar_val(0)\n",
    "    wiki.update_bar_max(len(sentences))\n",
    "    token_count = 0\n",
    "    # lemmatizing\n",
    "    wiki.update_text_status('Lemmatizing sentences...')\n",
    "    for i in range(len(sentences)):\n",
    "        tokens = word_tokenize(sentences[i]); token_count += len(tokens)\n",
    "        sentences[i] = ''.join([morph.parse(t)[0].normal_form + ' ' for t in tokens])\n",
    "        # widget visuals not on every step so we have no bottleneck in graphics\n",
    "        if i % bar_update_step == 0:\n",
    "            wiki.update_bar_val(i)\n",
    "            wiki.update_text_info(f\"{round(token_count/max(time()-b_time, 1), 2)} tokens/s\")\n",
    "            token_count = 0\n",
    "            b_time = time()\n",
    "    wiki.update_bar_done()\n",
    "    # saving to file\n",
    "    wiki.update_text_status(f\"Writing to '{output_file_name}'...\")\n",
    "    with open(output_file_name, 'a', encoding='utf-8') as f:\n",
    "        f.write(''.join([sent + \"\\n\" for sent in sentences]))\n",
    "        \n",
    "wiki.update_text_status(f\"Done!\")\n",
    "wiki.update_bar_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6e574f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee14f7ca860641f8bc4eefe613689187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "out = Output()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d917adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 22s\n",
      "Wall time: 20min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# example of practical using with wikipedia module only. No multithreads and generators\n",
    "from time import time, sleep\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from wikipedia import random as random_page, page\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "from requests.exceptions import RequestException\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def get_page(rec_depth=0):\n",
    "    if rec_depth >= 20:\n",
    "        raise RecursionError\n",
    "    page_title = random_page()\n",
    "    try:\n",
    "        return page(page_title).content\n",
    "    except WikipediaException:\n",
    "        return get_page(rec_depth=rec_depth + 1)\n",
    "    except RequestException:\n",
    "        sleep(5)\n",
    "        return get_page(rec_depth = rec_depth + 1)\n",
    "\n",
    "pages_amount = 500\n",
    "output_file_name = f'{pages_amount}_pages.txt'; open(output_file_name, 'w').close() # clearing/creating file\n",
    "update_step = 5\n",
    "\n",
    "for i in range(pages_amount):\n",
    "    if i % update_step == 0:\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"{round(100*i/pages_amount, 2)}%\")\n",
    "    pages_text = get_page()\n",
    "    # tokenizing\n",
    "    sentences = sent_tokenize(pages_text)\n",
    "    # lemmatizing\n",
    "    for i in range(len(sentences)):\n",
    "        tokens = word_tokenize(sentences[i])\n",
    "        sentences[i] = ''.join([morph.parse(t)[0].normal_form + ' ' for t in tokens])\n",
    "    # saving to file\n",
    "    with open(output_file_name, 'a', encoding='utf-8') as f:\n",
    "        f.write(''.join([sent + \"\\n\" for sent in sentences]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b35d",
   "metadata": {},
   "source": [
    "*результат налицо*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5dd18eb9130b2bdf528b55e6b9d0229966df3b79bb22d703689a1f050e5ef0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
